{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\n\nfrom tqdm import tqdm\nimport shutil\nimport pandas as pd\nimport numpy as np\nfrom argparse import Namespace\nfrom sklearn.model_selection import train_test_split\n\n\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_linear_schedule_with_warmup","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Set seed\nseed = 121\nnp.random.seed(seed)\ntorch.random.manual_seed(seed)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"'cuda'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Define Config"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"config = Namespace(\n    \n    train_file_path  = '../input/bag-of-words-meets-bags-of-popcorn-/labeledTrainData.tsv',\n    model_dir = 'models'\n)\n\n\nmodel_config = Namespace(\n    \n    model_name = 'bert-base-uncased',\n    max_len = 512,\n    batch_size = 8,\n    epochs = 3\n)","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## Create Folders\nif os.path.exists(config.model_dir):\n    shutil.rmtree(config.model_dir)\nos.makedirs(config.model_dir, exist_ok=True)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Load Data\ntrain_df = pd.read_csv(config.train_file_path, delimiter='\\t')","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Split data into train and val data\nX_train, X_val, y_train, y_val = train_test_split(train_df['review'].tolist(),\n                                                  train_df['sentiment'].tolist(), \n                                                  test_size = 0.3,\n                                                  random_state = 121)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train), len(y_train), len(X_val), len(y_val)","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"(17500, 17500, 7500, 7500)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentDataset(Dataset):\n    \n    def __init__(self, reviews, labels, tokenizer, max_len):\n        self.reviews = reviews\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __getitem__(self,index):\n        review, label = self.reviews[index], self.labels[index]\n        \n        encoding = self.tokenizer.encode_plus(\n                          review,\n                          add_special_tokens=True,\n                          max_length=self.max_len,\n                          return_token_type_ids=False,\n                          pad_to_max_length=True,\n                          return_attention_mask=True,\n                          return_tensors='pt',\n                        )\n        \n        return {\n                  'review_text': review,\n                  'input_ids': encoding['input_ids'].flatten(),\n                  'attention_mask': encoding['attention_mask'].flatten(),\n                  'label': torch.tensor(label, dtype=torch.long)\n                }\n        \n        \n    def __len__(self):\n        return len(self.reviews)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(model_config.model_name, do_lower_case = True)\ntrain_dataset = SentimentDataset(X_train, y_train, tokenizer, model_config.max_len)\nval_dataset = SentimentDataset(X_val, y_val, tokenizer, model_config.max_len)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataLoaderWrapper:\n    \n    def __init__(self, device, dataset, batch_size, shuffle):\n        self.device = device\n        self.dataset = dataset\n        self.dataset_size = len(dataset)\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        \n    def __iter__(self):\n        \n        dataloader = DataLoader(self.dataset,batch_size = self.batch_size,\n                                shuffle = self.shuffle, num_workers=4)\n    \n        for data in tqdm(dataloader, total = len(dataloader)):\n            ids = data['input_ids']\n            mask = data['attention_mask']\n            label = data['label']\n            yield ids.to(device),  mask.to(device), label.to(device)\n            \n    def get_number_of_batches(self):\n        return int(np.round(len(self.dataset)/self.batch_size,0))","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dl = DataLoaderWrapper(device, train_dataset, model_config.batch_size, True)\nval_dl = DataLoaderWrapper(device, val_dataset, model_config.batch_size, False)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = next(iter(train_dl))\n\nprint(f\"ids : {data[0].shape}\")\nprint(f\"mask : {data[1].shape}\")\nprint(f\"label : {data[2].shape}\")","execution_count":24,"outputs":[{"output_type":"stream","text":"  0%|          | 0/2188 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"ids : torch.Size([8, 512])\nmask : torch.Size([8, 512])\nlabel : torch.Size([8])\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Load Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n   \n    def __init__(self, modelname, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(modelname)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        last_hidden_state, pooled_output = self.bert(\n                                  input_ids=input_ids,\n                                  attention_mask=attention_mask\n                                )\n        output = self.drop(pooled_output)\n        output = self.out(output)\n        probs = F.softmax(output, dim = 1)\n        return probs","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model,dataloader,optimizer, loss_fn):\n        \n        ## training phase\n        model.train()\n        \n        running_loss = 0\n        correct = 0        \n        number_of_batches = dataloader.get_number_of_batches()\n        dataset_size = dataloader.dataset_size\n        \n       \n        for input_ids, attenstion_mask, label in dataloader:\n            \n            optimizer.zero_grad()\n            \n            probs = model(input_ids, attenstion_mask)\n            \n            loss = loss_fn(probs, label)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            pred_labels = torch.argmax(probs, dim = 1)\n            correct += torch.sum(pred_labels == label).item()\n        \n        epoch_running_loss = running_loss/number_of_batches\n        accuracy = correct/dataset_size\n        \n        return epoch_running_loss, accuracy\n\n    \ndef eval_fn(model,dataloader, loss_fn):\n        \n        ## validation phase\n        model.eval()\n        \n        running_loss = 0\n        correct = 0\n        number_of_batches = dataloader.get_number_of_batches()\n        dataset_size = dataloader.dataset_size\n        \n        with torch.no_grad():\n            for input_ids, attenstion_mask, label in dataloader:\n\n                probs = model(input_ids, attenstion_mask)     \n\n                loss = loss_fn(probs, label)\n                running_loss += loss.item()\n                pred_labels = torch.argmax(probs, dim = 1)\n\n                running_loss += loss.item()\n                correct += torch.sum(pred_labels == label).item()\n\n\n            epoch_running_loss = running_loss/number_of_batches\n            accuracy = correct/dataset_size\n        \n        return epoch_running_loss, accuracy\n    \n    \ndef run(model,train_dataloader,val_dataloader, epochs, optimizer,loss_fn, verbose = True):\n\n    print(\"Training Starts  \")\n    best_val_loss = 100000\n\n    for epoch in range(epochs):\n\n        ## Training\n        train_loss, train_acc = train_fn(model,train_dataloader, optimizer,loss_fn)\n        \n        ## Validation\n        val_loss, val_acc = eval_fn(model,val_dataloader, loss_fn)\n\n        if verbose: \n            loss_log = f\" Train loss: {train_loss:.3f}  Val loss: {val_loss:.3f}\"\n            acc_log = f\" Train accuracy : {train_acc:.3f}  Val accuarcy : {val_acc:.3f}\"\n            print(f\"Epoch : {epoch} \"+ loss_log + acc_log)\n\n\n        if best_val_loss > val_loss:\n            best_val_loss = val_loss\n            model_path = f\"models/epoch_{epoch}_best_val_loss_model.model\"\n            torch.save(model.state_dict(), model_path)\n","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# total_steps = len(train_dataset) * model_config.epochs\nmodel = SentimentClassifier(model_config.model_name,n_classes=2)\noptimizer = optim.Adam(model.parameters(), lr= 3e-6)\nloss_fn = nn.CrossEntropyLoss()\n\n## Training\nmodel.to(device)\nrun(model,train_dl, val_dl, model_config.epochs, optimizer, loss_fn)","execution_count":null,"outputs":[{"output_type":"stream","text":"\r  0%|          | 0/2188 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"Training Starts  \n","name":"stdout"},{"output_type":"stream","text":" 67%|██████▋   | 1458/2188 [10:48<05:31,  2.20it/s]","name":"stderr"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}