{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Search\n",
    "## LSH : Local Sensitive Hashing \n",
    "\n",
    "\n",
    "    - more efficient version of k-nearest neighbors using locality sensitive hashing\n",
    "    - You can use this to find the document search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "stop_words = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \n",
    "    # Remove URLS\n",
    "    clean_tweet = re.sub('https?:[\\/a-zA-Z.0-9]+','',tweet)\n",
    "    \n",
    "    #Convert @<something to <USR>\n",
    "    clean_tweet = re.sub('@[a-zA-Z0-9_-]+','<USR>',clean_tweet)\n",
    "    \n",
    "    # Remove '#' tags\n",
    "    clean_tweet = re.sub('#','', clean_tweet)\n",
    "    \n",
    "    # strip and lower\n",
    "    tokens = tokenizer.tokenize(clean_tweet)\n",
    "    tokens = [token.lower() for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_document_embedding(tweet, en_embedding):\n",
    "    \n",
    "    doc_embedding = np.zeros(300)\n",
    "    \n",
    "    for word in process_tweet(tweet):\n",
    "        if word in en_embedding:\n",
    "            doc_embedding += en_embedding[word]\n",
    "    return doc_embedding\n",
    "\n",
    "def get_document_vecs(all_docs, en_embedding):\n",
    "    \n",
    "    index2doc_dict = {}\n",
    "    document_vec_list = []\n",
    "    \n",
    "    for i,doc in enumerate(all_docs):\n",
    "        doc_embedding = get_document_embedding(doc, en_embedding)\n",
    "        index2doc_dict[i] = doc_embedding\n",
    "        document_vec_list.append(doc_embedding)\n",
    "    \n",
    "    document_vec_matrix = np.vstack(document_vec_list)\n",
    "    \n",
    "    return document_vec_matrix, index2doc_dict\n",
    "\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    dot = np.dot(a,b)\n",
    "    score = dot/(norm_a*norm_b)\n",
    "    \n",
    "    if pd.isnull(score):\n",
    "        score = 0\n",
    "    return score\n",
    "\n",
    "def nearnest_neighbors(a, candidates):\n",
    "    scores = []\n",
    "    \n",
    "    for b in candidates:\n",
    "        scores.append(cosine_similarity(a,b))\n",
    "    \n",
    "    idx = np.argmax(scores)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6370"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = positive_tweets + negative_tweets\n",
    "\n",
    "## Load subset embedding\n",
    "en_embeddings = pickle.load(open('./subset_embedding/en_embeddings.p','rb'))\n",
    "len(en_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 10000\n",
      "shape of document_vecs (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings)\n",
    "print(f\"length of dictionary {len(ind2Tweet)}\")\n",
    "print(f\"shape of document_vecs {document_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Similar tweets\n",
    "\n",
    "    - To find out similar tweet, we will check all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet : i am sad\n",
      "Similiar Tweet : @hanbined sad pray for me :(((\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'i am sad'\n",
    "process_tweet(my_tweet)\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings)\n",
    "\n",
    "idx = nearnest_neighbors(tweet_embedding, document_vecs)\n",
    "print(f\"Tweet : {my_tweet}\")\n",
    "print(f\"Similiar Tweet : {all_tweets[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Similar tweets using LSH(Local Sensitive hashing)\n",
    "\n",
    "    - Instead of looking all vectors(tweets), you can just search a subset to find its neighbnors.\n",
    "    - You can divide the vector space into regions and search within one region for nearest neighbors of a given vector.\n",
    "    \n",
    "    \n",
    "  #### Choosing the number of planes\n",
    "\n",
    "* Each plane divides the space to $2$ parts.\n",
    "* So $n$ planes divide the space into $2^{n}$ hash buckets.\n",
    "* We want to organize 10,000 document vectors into buckets so that every bucket has about $~16$ planes.\n",
    "* For that we need $\\frac{10000}{16}=625$ buckets.\n",
    "* We're interested in $n$, number of planes, so that $2^{n}= 625$. Now, we can calculate $n=\\log_{2}625 = 9.29 \\approx 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VECS = len(all_tweets)     # number of vectors\n",
    "N_DIMS = len(ind2Tweet[0])    # dim of vectors\n",
    "N_PLANES = 10\n",
    "N_UNIVERSES = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES)) for _ in range(N_UNIVERSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1):\n",
    "   \n",
    "    similarity_l = []\n",
    "\n",
    "    for row in candidates:\n",
    "        cos_similarity = cosine_similarity(v,row)\n",
    "\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "\n",
    "    dot_product = np.dot(v,planes)    \n",
    "    sign_of_dot_product = np.sign(dot_product)\n",
    "    \n",
    "    h = sign_of_dot_product>=0\n",
    "    h = np.squeeze(h)\n",
    "\n",
    "    hash_value = 0\n",
    "    n_planes = planes.shape[1]\n",
    "    \n",
    "    for i in range(n_planes):\n",
    "        hash_value += np.power(2,i)*h[i]\n",
    "    hash_value = int(hash_value)\n",
    "\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes):\n",
    "   \n",
    "    num_of_planes = planes.shape[1]\n",
    "\n",
    "    num_buckets = 2**num_of_planes\n",
    "\n",
    "    hash_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    id_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    for i, v in enumerate(vecs):\n",
    "\n",
    "        h = hash_value_of_vector(v,planes)\n",
    "        hash_table[h].append(v)\n",
    "\n",
    "        id_table[h].append(i)\n",
    "\n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on hash universe #: 0\n",
      "working on hash universe #: 1\n",
      "working on hash universe #: 2\n",
      "working on hash universe #: 3\n",
      "working on hash universe #: 4\n",
      "working on hash universe #: 5\n",
      "working on hash universe #: 6\n",
      "working on hash universe #: 7\n",
      "working on hash universe #: 8\n",
      "working on hash universe #: 9\n",
      "working on hash universe #: 10\n",
      "working on hash universe #: 11\n",
      "working on hash universe #: 12\n",
      "working on hash universe #: 13\n",
      "working on hash universe #: 14\n",
      "working on hash universe #: 15\n",
      "working on hash universe #: 16\n",
      "working on hash universe #: 17\n",
      "working on hash universe #: 18\n",
      "working on hash universe #: 19\n",
      "working on hash universe #: 20\n",
      "working on hash universe #: 21\n",
      "working on hash universe #: 22\n",
      "working on hash universe #: 23\n",
      "working on hash universe #: 24\n"
     ]
    }
   ],
   "source": [
    "# Creating the hashtables\n",
    "hash_tables = []\n",
    "id_tables = []\n",
    "for universe_id in range(N_UNIVERSES):  # there are 25 hashes\n",
    "    print('working on hash universe #:', universe_id)\n",
    "    planes = planes_l[universe_id]\n",
    "    hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_knn(doc_id, v, planes_l, k=1, num_universes_to_use=N_UNIVERSES):\n",
    "\n",
    "    vecs_to_consider_l = list()\n",
    "\n",
    "    ids_to_consider_l = list()\n",
    "\n",
    "    ids_to_consider_set = set()\n",
    "\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "\n",
    "        planes = planes_l[universe_id]\n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "        hash_table = hash_tables[universe_id]\n",
    "        document_vectors_l = hash_table[hash_value]\n",
    "        id_table = id_tables[universe_id]\n",
    "        new_ids_to_consider = id_table[hash_value]\n",
    "\n",
    "        if doc_id in new_ids_to_consider:\n",
    "            new_ids_to_consider.remove(doc_id)\n",
    "            print(f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n",
    "\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                document_vector_at_i = document_vectors_l[i]\n",
    "                vecs_to_consider_l.append(document_vector_at_i)\n",
    "                ids_to_consider_l.append(new_id)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "\n",
    "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
    "\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "\n",
    "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "    print(nearest_neighbor_idx_l)\n",
    "    print(ids_to_consider_l)\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx] for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "    return nearest_neighbor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_vecs, ind2Tweet\n",
    "doc_id = 0\n",
    "doc_to_search = all_tweets[doc_id]\n",
    "vec_to_search = document_vecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "Fast considering 186 vecs\n",
      "[82 16  0]\n",
      "[51, 105, 154, 195, 253, 615, 1111, 1959, 2176, 2240, 3319, 3715, 4050, 6338, 8497, 9168, 1803, 2858, 1063, 1657, 1789, 2366, 2598, 2773, 3280, 4736, 5642, 7023, 7985, 8359, 8556, 9296, 10, 26, 63, 69, 113, 164, 209, 272, 288, 604, 911, 1153, 1494, 1783, 2031, 2367, 2554, 2799, 2872, 2905, 3018, 3068, 3608, 3734, 3784, 3962, 4369, 4463, 4748, 4955, 5037, 5318, 5803, 5848, 5923, 6442, 6814, 6824, 6994, 7310, 7987, 8090, 8846, 9077, 9584, 9630, 9739, 9, 122, 124, 268, 332, 360, 532, 542, 555, 657, 705, 741, 829, 1117, 1215, 1332, 1384, 1619, 1720, 1912, 2034, 2172, 2355, 2411, 2414, 2480, 2606, 2718, 2750, 2940, 2945, 2991, 3079, 3251, 3338, 3437, 3498, 3685, 4023, 4086, 4349, 4862, 4907, 4908, 4933, 4939, 4967, 5145, 5363, 5395, 5402, 5469, 5564, 5653, 5686, 5822, 5956, 6103, 6182, 6195, 6267, 6585, 6644, 6693, 6858, 6955, 7129, 7162, 7269, 7363, 7404, 7432, 7556, 7598, 7610, 7801, 7909, 7962, 8081, 8297, 8358, 8450, 8504, 8505, 8535, 8573, 8593, 8597, 8640, 8667, 8723, 8814, 8844, 8967, 9055, 9083, 9121, 9133, 9159, 9354, 9519, 9701, 9719, 9784, 9814, 9871, 9886]\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbor_ids = approximate_knn(doc_id, vec_to_search, planes_l, k=3, num_universes_to_use=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for document 0\n",
      "Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Nearest neighbor at document id 268\n",
      "document contents: @imarpita it was great talking to you :D\n",
      "Nearest neighbor at document id 1803\n",
      "document contents: @americascup Do you have stage times for Portsmouth live? :) x\n",
      "Nearest neighbor at document id 51\n",
      "document contents: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors for document {doc_id}\")\n",
    "print(f\"Document contents: {doc_to_search}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {all_tweets[neighbor_id]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
