{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective : English to french translation\n",
    "\n",
    "\n",
    "## Algorithm \n",
    "\n",
    "    1. Suppose X and Y are vector space of english and french respectively.\n",
    "    2. If we multiply R with X we will get Y. i.e. X * R = Y\n",
    "    3. Once you are in french vector space, you will have to find the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## english to frech dictionaries\n",
    "config = Namespace(\n",
    "    train_file_path = './data/en-fr.train.txt',\n",
    "    test_file_path = './data/en-fr.test.txt',\n",
    "    \n",
    "    english_embedding_path = './subset_embedding/en_embeddings.p',\n",
    "    french_embedding_path = './subset_embedding/fr_embeddings.p'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6370, 5766)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Subset of embedding\n",
    "en_embedding  = pickle.load(open(config.english_embedding_path, \"rb\"))\n",
    "fr_embedding  = pickle.load(open(config.french_embedding_path, \"rb\"))\n",
    "\n",
    "\n",
    "len(en_embedding), len(fr_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = []\n",
    "\n",
    "with open(config.train_file_path) as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split(' ')\n",
    "        if len(line)==2:\n",
    "            train_pairs.append([line[0], line[1]])\n",
    "            \n",
    "            \n",
    "test_pairs = []\n",
    "\n",
    "with open(config.test_file_path) as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split(' ')\n",
    "        if len(line)==2:\n",
    "            test_pairs.append([line[0], line[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10872, 2943)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pairs), len(test_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X,y,R):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    gradient = np.dot(X.T, np.dot(X,R) - y) * (2/m)\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def compute_loss(X,y,R):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    diff = np.dot(X,R) - y\n",
    "    diff_squared = diff**2\n",
    "    sum_diff_suared = np.sum(diff_squared)\n",
    "    \n",
    "    loss = sum_diff_suared/m\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def get_embedding(word, embedding_mat):\n",
    "    if word in embedding_mat:\n",
    "        return embedding_mat[word].reshape(1,300)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrics(pairs, en_embedding, fr_embedding):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i,(en_word, fr_word) in enumerate(train_pairs):\n",
    "        en_word_embedd = get_embedding(en_word,en_embedding)\n",
    "        fr_word_embedd = get_embedding(fr_word, fr_embedding)\n",
    "\n",
    "        if en_word_embedd is not None and fr_word_embedd is not None:\n",
    "            x_list.append(en_word_embedd)\n",
    "            y_list.append(fr_word_embedd)\n",
    "\n",
    "    X = np.vstack(x_list)\n",
    "    y= np.vstack(y_list)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_matrics(train_pairs, en_embedding, fr_embedding)\n",
    "X_val, y_val = get_matrics(test_pairs, en_embedding, fr_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Loss : 103.75080712125232\n",
      "Epoch 49 Loss : 28.581241208690365\n",
      "Epoch 74 Loss : 10.52838409224959\n",
      "Epoch 99 Loss : 4.740614270409274\n",
      "Epoch 124 Loss : 2.525009208180008\n",
      "Epoch 149 Loss : 1.5630449512086788\n",
      "Epoch 174 Loss : 1.1048770444767242\n",
      "Epoch 199 Loss : 0.8710846200438042\n",
      "Epoch 224 Loss : 0.745354168029209\n",
      "Epoch 249 Loss : 0.6748854542077934\n",
      "Epoch 274 Loss : 0.6340361932715131\n",
      "Epoch 299 Loss : 0.6096758141723325\n",
      "Epoch 324 Loss : 0.5947892102152811\n",
      "Epoch 349 Loss : 0.5854952917369657\n",
      "Epoch 374 Loss : 0.5795823131084801\n",
      "Epoch 399 Loss : 0.5757569140300391\n"
     ]
    }
   ],
   "source": [
    "## Optimizer R\n",
    "def train(epochs , lr):\n",
    "    \n",
    "    np.random.seed(129)\n",
    "    \n",
    "    R = np.random.rand(300, 300)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss = compute_loss(X_train,y_train,R)\n",
    "        gradient = compute_gradient(X_train,y_train,R)\n",
    "        R = R - (lr * gradient)\n",
    "\n",
    "        if (epoch+1)%25 == 0:\n",
    "            print(f\"Epoch {epoch} Loss : {loss}\")\n",
    "            \n",
    "    return R\n",
    "\n",
    "\n",
    "R = train(epochs = 400, lr = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    fr_pred_emb = np.dot(x,R)\n",
    "    return fr_pred_emb\n",
    "\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    cos = -10\n",
    "    dot = np.dot(A, B)\n",
    "    norma = np.linalg.norm(A)\n",
    "    normb = np.linalg.norm(B)\n",
    "    cos = dot / (norma * normb)\n",
    "    return cos\n",
    "\n",
    "\n",
    "def nearest_neighbor(v, candidates, k = 1):\n",
    "    \n",
    "    similarity_l = []\n",
    "\n",
    "    for row in candidates:\n",
    "        cos_similarity = cosine_similarity(v,row)\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_vocabulary(X, Y, R):\n",
    "   \n",
    "    pred = np.dot(X,R)\n",
    "\n",
    "    num_correct = 0\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        pred_idx = nearest_neighbor(pred[i],Y)\n",
    "\n",
    "        if pred_idx == i:\n",
    "            num_correct += 1\n",
    "\n",
    "    accuracy = num_correct / len(pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.450\n"
     ]
    }
   ],
   "source": [
    "acc = test_vocabulary(X_val, y_val, R)  # this might take a minute or two\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You managed to translate words from one language to another language without ever seing them with almost 45% accuracy by using some basic linear algebra and learning a mapping of words from one language to another!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
