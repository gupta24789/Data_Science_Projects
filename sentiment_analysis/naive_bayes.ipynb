{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from argparse import Namespace\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    train_file_path = './data/raw_data/labeledTrainData.tsv',\n",
    "    test_file_path = './data/raw_data/testData.tsv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data\n",
    "train_df = pd.read_csv(args.train_file_path, delimiter='\\t')\n",
    "test_df = pd.read_csv(args.test_file_path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data into train and val data\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df['review'].tolist(),train_df['sentiment'].tolist(), test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17500, 17500, 7500, 7500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train), len(X_val), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "remove_words = string.punctuation + '0123456789'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review(review):\n",
    "    \n",
    "    review = BeautifulSoup(review).get_text()    \n",
    "    review = re.sub('^\\w+','', review)\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "    clean_tokens = [lemmatizer.lemmatize(w.lower()) for w in tokens if w not in stop_words and w not in remove_words if w.isalpha()]\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freq(reviews, labels, threshold = 2):\n",
    "    freqs = defaultdict(int)\n",
    "    \n",
    "    for review, label in tqdm.tqdm(zip(reviews, labels),total = len(reviews)):\n",
    "        clean_review = process_review(review)\n",
    "        for w in clean_review:\n",
    "            freqs[(w,label)] += 1\n",
    "            \n",
    "    freqs = {w:v for w,v in freqs.items() if v>threshold}\n",
    "                \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17500/17500 [01:40<00:00, 174.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37914"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build Vocab\n",
    "freqs = build_freq(X_train, y_train)\n",
    "len(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37914/37914 [02:57<00:00, 214.12it/s]\n"
     ]
    }
   ],
   "source": [
    "## Calculate Probability\n",
    "probs_df = pd.DataFrame()\n",
    "for k,v in tqdm.tqdm(freqs.items()):\n",
    "    probs_df.loc[k[0],k[1]] = v\n",
    "probs_df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_df.rename(columns = {0:'pos_freq',1:'neg_freq'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize probability using laplacian smoothing\n",
    "total_pos_freq = probs_df['pos_freq'].sum()\n",
    "total_neg_freq = probs_df['neg_freq'].sum()\n",
    "vocab_size = probs_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_df['pos_prob'] = probs_df[0].apply(lambda x: (x+1)/(vocab_size+total_pos_freq))\n",
    "probs_df['neg_prob'] = probs_df[1].apply(lambda x: (x+1)/(vocab_size+total_neg_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>0.017342</td>\n",
       "      <td>1.335375e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>0.001716</td>\n",
       "      <td>1.174750e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worse</th>\n",
       "      <td>0.000732</td>\n",
       "      <td>1.286584e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.027639</td>\n",
       "      <td>2.275174e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ever</th>\n",
       "      <td>0.002141</td>\n",
       "      <td>1.710166e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffered</th>\n",
       "      <td>0.000050</td>\n",
       "      <td>5.047366e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolute</th>\n",
       "      <td>0.000115</td>\n",
       "      <td>1.009473e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.005767</td>\n",
       "      <td>6.115230e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got</th>\n",
       "      <td>0.001418</td>\n",
       "      <td>1.138132e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrible</th>\n",
       "      <td>0.000767</td>\n",
       "      <td>1.474623e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forced</th>\n",
       "      <td>0.000234</td>\n",
       "      <td>1.860598e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dialogue</th>\n",
       "      <td>0.000617</td>\n",
       "      <td>3.839957e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pointless</th>\n",
       "      <td>0.000274</td>\n",
       "      <td>1.880391e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plot</th>\n",
       "      <td>0.002647</td>\n",
       "      <td>1.522127e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>development</th>\n",
       "      <td>0.000236</td>\n",
       "      <td>1.395448e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>0.004465</td>\n",
       "      <td>3.732082e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drawn</th>\n",
       "      <td>0.000110</td>\n",
       "      <td>1.316274e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imagery</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>6.037046e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene</th>\n",
       "      <td>0.003546</td>\n",
       "      <td>3.019513e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look</th>\n",
       "      <td>0.002572</td>\n",
       "      <td>1.814083e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.007841</td>\n",
       "      <td>5.996469e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>0.000616</td>\n",
       "      <td>6.314156e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>school</th>\n",
       "      <td>0.000515</td>\n",
       "      <td>4.631701e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remedial</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>art</th>\n",
       "      <td>0.000396</td>\n",
       "      <td>5.037469e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>project</th>\n",
       "      <td>0.000197</td>\n",
       "      <td>1.484519e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horror</th>\n",
       "      <td>0.001337</td>\n",
       "      <td>8.758665e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0.001326</td>\n",
       "      <td>2.720629e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attractive</th>\n",
       "      <td>0.000120</td>\n",
       "      <td>9.302988e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>0.001481</td>\n",
       "      <td>1.450870e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tuskan</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nel</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pekinpah</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hollister</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itier</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daggett</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hessler</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tira</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.958718e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bulimic</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.958718e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yamadera</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.958718e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasalle</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>6.927757e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enix</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.958718e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markov</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>komomo</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>springwood</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>7.917437e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wimpiest</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newsweek</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>erroll</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nakadei</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.958718e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ryoko</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>4.948398e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>klemper</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.958718e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deterrent</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.958718e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>professing</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.958718e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airball</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kraakman</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patrica</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perú</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brommell</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.958718e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackbuster</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sayori</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.896796e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23947 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0             1\n",
       "movie        0.017342  1.335375e-02\n",
       "actually     0.001716  1.174750e-03\n",
       "worse        0.000732  1.286584e-04\n",
       "i            0.027639  2.275174e-02\n",
       "ever         0.002141  1.710166e-03\n",
       "suffered     0.000050  5.047366e-05\n",
       "absolute     0.000115  1.009473e-04\n",
       "it           0.005767  6.115230e-03\n",
       "got          0.001418  1.138132e-03\n",
       "terrible     0.000767  1.474623e-04\n",
       "forced       0.000234  1.860598e-04\n",
       "dialogue     0.000617  3.839957e-04\n",
       "pointless    0.000274  1.880391e-05\n",
       "plot         0.002647  1.522127e-03\n",
       "development  0.000236  1.395448e-04\n",
       "really       0.004465  3.732082e-03\n",
       "drawn        0.000110  1.316274e-04\n",
       "imagery      0.000048  6.037046e-05\n",
       "scene        0.003546  3.019513e-03\n",
       "look         0.002572  1.814083e-03\n",
       "like         0.007841  5.996469e-03\n",
       "high         0.000616  6.314156e-04\n",
       "school       0.000515  4.631701e-04\n",
       "remedial     0.000005  9.896796e-07\n",
       "art          0.000396  5.037469e-04\n",
       "project      0.000197  1.484519e-04\n",
       "horror       0.001337  8.758665e-04\n",
       "best         0.001326  2.720629e-03\n",
       "attractive   0.000120  9.302988e-05\n",
       "woman        0.001481  1.450870e-03\n",
       "...               ...           ...\n",
       "tuskan       0.000004  9.896796e-07\n",
       "nel          0.000007  9.896796e-07\n",
       "pekinpah     0.000004  9.896796e-07\n",
       "hollister    0.000004  9.896796e-07\n",
       "itier        0.000004  9.896796e-07\n",
       "daggett      0.000005  9.896796e-07\n",
       "hessler      0.000005  9.896796e-07\n",
       "tira         0.000001  3.958718e-06\n",
       "bulimic      0.000001  3.958718e-06\n",
       "yamadera     0.000001  3.958718e-06\n",
       "lasalle      0.000001  6.927757e-06\n",
       "enix         0.000001  3.958718e-06\n",
       "markov       0.000006  9.896796e-07\n",
       "komomo       0.000004  9.896796e-07\n",
       "springwood   0.000001  7.917437e-06\n",
       "wimpiest     0.000004  9.896796e-07\n",
       "newsweek     0.000004  9.896796e-07\n",
       "erroll       0.000013  9.896796e-07\n",
       "nakadei      0.000001  3.958718e-06\n",
       "ryoko        0.000001  4.948398e-06\n",
       "klemper      0.000001  3.958718e-06\n",
       "deterrent    0.000001  3.958718e-06\n",
       "professing   0.000001  3.958718e-06\n",
       "airball      0.000004  9.896796e-07\n",
       "kraakman     0.000004  9.896796e-07\n",
       "patrica      0.000004  9.896796e-07\n",
       "perú         0.000009  9.896796e-07\n",
       "brommell     0.000001  3.958718e-06\n",
       "blackbuster  0.000004  9.896796e-07\n",
       "sayori       0.000004  9.896796e-07\n",
       "\n",
       "[23947 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naive bayes\n",
    "## log_likihood = logprior + log(pos/neg)\n",
    "## if log_likihood > 0 then 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Features\n",
    "X_train_features = list(map(lambda x: create_feature(x, freqs),X_train))\n",
    "X_train_features = np.vstack(X_train_features)\n",
    "y_train_features = np.array(y_train)\n",
    "\n",
    "## Val fetaures\n",
    "X_val_features = list(map(lambda x: create_feature(x, freqs),X_val))\n",
    "X_val_features = np.vstack(X_val_features)\n",
    "y_val_features = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_features,y_train_features)\n",
    "\n",
    "print(f\"Train Accuracy : {model.score(X_train_features,y_train_features)}\")\n",
    "print(f\"Val Accuracy : {model.score(X_val_features,y_val_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test fetaures\n",
    "X_test_features = list(map(lambda x: create_feature(x, freqs),test_df.review.tolist()))\n",
    "X_test_features = np.vstack(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test_features)\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
